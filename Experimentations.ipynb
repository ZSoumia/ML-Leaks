{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Leaks Paper implementation:\n",
    "\n",
    ">This is an implementation of adversary 1 scenario of the paper (ML-Leaks)[https://arxiv.org/pdf/1806.01246.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. General Idea of the paper:\n",
    "\n",
    "As machine learning as a service (Mlaas) is getting widely used, privacy issues in this setting are also getting a lot of attention. One of the most famous attacks that a depolyed machine learning model can be victim of is \"the membership inference attack\". This attack allows an adversary to know if a particular data point was used to trained a given model. In case of a model trained on highly private data, this attack can be a real danger.\n",
    "\n",
    "Early demos of memebership infrence attacks assumed the following :\n",
    "\n",
    "- Knowledge of the target model architecture.\n",
    "\n",
    "- Using multiple shadow models.\n",
    "\n",
    "- Having shadow models trained on datasets that are from the same distribution of the dataset used to train the target model itself.\n",
    "\n",
    "Despite the high performance of such attacks, the existence of these assumptions made it unrealistic to be deployed in real world. In this paper, the authors relaxed these assumptions and suggested some effective defense strategies , namely : Dropout and model stacking.\n",
    "\n",
    "The authors designed 3 adversary strategies :\n",
    "\n",
    "- Adversary 1 : Used only one shadow model + no knowledge of the classification algorithm used. ==> The usage of only one shadow model resulted in a similar performance compared to using many shadow models. This can make the attacks much more computationally efficient.\n",
    "\n",
    "- Adversary 2 : The data sued to train the shadow model isn't necessarily from the same distribution of the data used to train the target model ==> This makes the attack more powerful and realistic.\n",
    "\n",
    "- Adversary 3 : No shadow models are used + an attack in an unsupervised attack ==> despite the drop of the performance for this attck it is still effective.\n",
    "\n",
    "> In this notebook we are implementing the Adversary 1 and testing on Mnist dataset and Cifar10.\n",
    "\n",
    "### Table of Content:\n",
    "<ol>\n",
    "    <li> <a href=\"#case1\">Case 1 : using the same architecture for the target and the shadow</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#mnist1\">Mnist study case</a></li>\n",
    "            <li><a href=\"#dataloaders\">Load data</a></li>\n",
    "            <li><a href=\"#training\">Training models</a></li>\n",
    "            <li><a href=\"#eval\">Some Extra evaluation results</a></li>\n",
    "        </ol>\n",
    "     </li>\n",
    "    <li> <a href=\"#mnist\">Cifar10 dataset case</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#dataloaders2\">Load data</a></li>\n",
    "            <li><a href=\"#training2\">Training models</a></li>\n",
    "            <li><a href=\"#eval2\">Some Extra evaluation results</a></li>\n",
    "        </ol>\n",
    "     </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important note : The number of epochs displayed when training may not be the exact number of training epochs used for the final model. By the time I was running tests , it happened when I needed to add epochs and run again the training cell ( to improuve the accuracy/loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages \n",
    "from models.models import *\n",
    "import dataloaders as loader\n",
    "import training_utils as TU\n",
    "from torch.optim import *\n",
    "from datasets.data_utils import *\n",
    "from datasets.case2_data_utils import *\n",
    "from datasets.attack_dataset import *  \n",
    "import pandas as pd \n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Case 1 : Same model architecture for the shadow and the target <a id=\"case1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Mnist classifier: <a id =\"mnist1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Load data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = loader.get_dataloaders(batch_size=128,dataset_name=\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DShadow_train', 'DShadow_out', 'target_train', 'target_eval', 'test_loader'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 train and evaluate the target model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Mnist_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 1.7336031607652114\n",
      "Epoch 2 - Training loss: 0.6991646052417109\n",
      "Epoch 3 - Training loss: 0.43768555153224425\n",
      "Epoch 4 - Training loss: 0.3306552916765213\n",
      "Epoch 5 - Training loss: 0.2674894731933788\n",
      "Epoch 6 - Training loss: 0.22669988011909745\n",
      "Epoch 7 - Training loss: 0.19536167168516225\n",
      "Epoch 8 - Training loss: 0.17107124607694352\n",
      "Epoch 9 - Training loss: 0.15237095478480145\n",
      "Epoch 10 - Training loss: 0.13591645995817953\n",
      "Epoch 11 - Training loss: 0.12330653928851677\n",
      "Epoch 12 - Training loss: 0.1126803884251138\n",
      "Epoch 13 - Training loss: 0.10286486606602951\n",
      "Epoch 14 - Training loss: 0.09479775257661181\n",
      "Epoch 15 - Training loss: 0.08915792882316194\n",
      "Epoch 16 - Training loss: 0.08216708196106098\n",
      "Epoch 17 - Training loss: 0.07772603141680612\n",
      "Epoch 18 - Training loss: 0.0728833022195909\n",
      "Epoch 19 - Training loss: 0.06714723495987512\n",
      "Epoch 20 - Training loss: 0.0630572308012742\n",
      "Epoch 21 - Training loss: 0.059463714375713114\n",
      "Epoch 22 - Training loss: 0.055863706426600275\n",
      "Epoch 23 - Training loss: 0.052924576207553434\n",
      "Epoch 24 - Training loss: 0.05075882353930403\n",
      "Epoch 25 - Training loss: 0.04710791998353424\n",
      "Epoch 26 - Training loss: 0.045075532830304514\n",
      "Epoch 27 - Training loss: 0.04265704567117964\n",
      "Epoch 28 - Training loss: 0.040445889527830535\n",
      "Epoch 29 - Training loss: 0.03859937981844454\n",
      "Epoch 30 - Training loss: 0.03658336338634461\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = loaders['target_train']\n",
    "optimizer = Adam(target_model.parameters(), lr=0.0001)\n",
    "epochs = 30\n",
    "checkpoint_path = \"./checkpoints/case1/mnistTarget.pth\"\n",
    "target_train_loss_scores = TU.train(train_dataloader=train_dataloader, \n",
    "                                    optimizer=optimizer,\n",
    "                                    model=target_model,\n",
    "                                    n_epochs=epochs,\n",
    "                                    model_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.0004, Val Accuracy: 9846/10000 (98.460%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "acc = TU.eval_model(target_model,loaders['test_loader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results \n",
    "TU.save_training_loss(target_train_loss_scores,\"case1/target_model_train_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 train and evaluate the shadow model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_model = Mnist_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 1.650734110403869\n",
      "Epoch 2 - Training loss: 0.6618791730222056\n",
      "Epoch 3 - Training loss: 0.4201601612365852\n",
      "Epoch 4 - Training loss: 0.31804574590365764\n",
      "Epoch 5 - Training loss: 0.25947067652971056\n",
      "Epoch 6 - Training loss: 0.22189391688522647\n",
      "Epoch 7 - Training loss: 0.19272407453696608\n",
      "Epoch 8 - Training loss: 0.16948357173952006\n",
      "Epoch 9 - Training loss: 0.15162894909538455\n",
      "Epoch 10 - Training loss: 0.13634572556968463\n",
      "Epoch 11 - Training loss: 0.12467115845973209\n",
      "Epoch 12 - Training loss: 0.1144538798953517\n",
      "Epoch 13 - Training loss: 0.1062625774766429\n",
      "Epoch 14 - Training loss: 0.09781695144661402\n",
      "Epoch 15 - Training loss: 0.09179458075787052\n",
      "Epoch 16 - Training loss: 0.08455126115389294\n",
      "Epoch 17 - Training loss: 0.07867810913061692\n",
      "Epoch 18 - Training loss: 0.07313994658416358\n",
      "Epoch 19 - Training loss: 0.06843136344105005\n",
      "Epoch 20 - Training loss: 0.06478635789984363\n",
      "Epoch 21 - Training loss: 0.061030569454749765\n",
      "Epoch 22 - Training loss: 0.05771656714821771\n",
      "Epoch 23 - Training loss: 0.05420634732187047\n",
      "Epoch 24 - Training loss: 0.05107042243135935\n",
      "Epoch 25 - Training loss: 0.04791683941227147\n",
      "Epoch 26 - Training loss: 0.0456683318746292\n",
      "Epoch 27 - Training loss: 0.04273505269754994\n",
      "Epoch 28 - Training loss: 0.041465378397980986\n",
      "Epoch 29 - Training loss: 0.03880343389681588\n",
      "Epoch 30 - Training loss: 0.03650453166594192\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = loaders['DShadow_train']\n",
    "optimizer = Adam(shadow_model.parameters(), lr=0.0001)\n",
    "epochs = 30\n",
    "checkpoint_path = \"./checkpoints/case1/mnistShadow.pth\"\n",
    "shadow_train_loss_scores = TU.train(train_dataloader=train_dataloader, \n",
    "                                    optimizer=optimizer,\n",
    "                                    model=shadow_model,\n",
    "                                    n_epochs=epochs,\n",
    "                                    model_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.0004, Val Accuracy: 9825/10000 (98.250%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_shadow = TU.eval_model(shadow_model,loaders['test_loader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results \n",
    "TU.save_training_loss(shadow_train_loss_scores,\"case1/shadow_model_train_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 train and evaluate the attack model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.1 Create dataset of the attack model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The dataset is already created and saved in data/data/attack_dataset.csv so there s no need \n",
    "# to run this cell \n",
    "train_file_path='data/case1/mnist/attack_train_dataset.csv'\n",
    "test_file_path='data/case1/mnist/attack_test_dataset.csv'\n",
    "# In short the fowllong means that:\n",
    "#  Attack_train = top3(shadow(D_shadow)) = top3(shadow(D_shadwo_train + D_shadow_out_shadow)) \n",
    "result = get_attack_trainset(shadow_model=shadow_model,\n",
    "                             shadow_train_loader=loaders['DShadow_train'], \n",
    "                             shadow_out_loader=loaders['DShadow_out'],\n",
    "                             file_path=train_file_path)\n",
    "result = get_attack_trainset(shadow_model=shadow_model,\n",
    "                             shadow_train_loader=loaders['target_train'], \n",
    "                             shadow_out_loader=loaders['target_eval'],\n",
    "                             file_path=test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path='data/case1/mnist/attack_train_dataset.csv'\n",
    "test_file_path='data/case1/mnist/attack_test_dataset.csv'\n",
    "attack_loaders = loader.get_attack_train_test_loaders(dataset_train_path=train_file_path,\n",
    "                                                      dataset_test_path=test_file_path,\n",
    "                                                        batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.2 train and evaluate the attack model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_model = Attack_classifier(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = attack_loaders['train_loader']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.6961096758445104\n",
      "Epoch 2 - Training loss: 0.6956390447219213\n",
      "Epoch 3 - Training loss: 0.6959529554247856\n",
      "Epoch 4 - Training loss: 0.6956756233970324\n",
      "Epoch 5 - Training loss: 0.6961275424361228\n",
      "Epoch 6 - Training loss: 0.6961167560418446\n",
      "Epoch 7 - Training loss: 0.696594585955143\n",
      "Epoch 8 - Training loss: 0.6959110643863679\n",
      "Epoch 9 - Training loss: 0.6963876638611157\n",
      "Epoch 10 - Training loss: 0.6960333439906439\n",
      "Epoch 11 - Training loss: 0.6956823712587357\n",
      "Epoch 12 - Training loss: 0.6963086466987928\n",
      "Epoch 13 - Training loss: 0.6951838530699412\n",
      "Epoch 14 - Training loss: 0.6957121727267901\n",
      "Epoch 15 - Training loss: 0.6960745484630266\n",
      "Epoch 16 - Training loss: 0.6964540767272314\n",
      "Epoch 17 - Training loss: 0.6954973247647286\n",
      "Epoch 18 - Training loss: 0.6959402126471201\n",
      "Epoch 19 - Training loss: 0.6955236923098564\n",
      "Epoch 20 - Training loss: 0.6957927646835645\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(attack_model.parameters(), lr=0.00001)\n",
    "\n",
    "train_scores= TU.train(train_dataloader, \n",
    "                    optimizer,\n",
    "                    model=attack_model,\n",
    "                    n_epochs=20, \n",
    "                    model_path=\"./checkpoints/case1/mnist_attack_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.0693, Val Accuracy: (0.499%) , precision:  0.4994, recall :  0.8363\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_attack,precision_attack,recall_attack = TU.eval_model(attack_model,attack_loaders['test_loader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results \n",
    "TU.save_training_loss(train_scores,\"case1/attack_model_train_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "raw_data = pd.read_csv('data/attack_train_dataset.csv')\n",
    "x = raw_data[['top1', 'top2', 'top3']]\n",
    "y = raw_data['label']\n",
    "test_data = pd.read_csv(\"data/attack_test_dataset.csv\")\n",
    "x_test = test_data[['top1', 'top2', 'top3']]\n",
    "y_test = test_data['label']\n",
    "model = LogisticRegression()\n",
    "model.fit(x, y)\n",
    "predictions = model.predict(x_test)\n",
    "print(len(predictions))\n",
    "l = classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cifar10 classifier :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Load data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_loaders = loader.get_dataloaders(batch_size=10,dataset_name=\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DShadow_train', 'DShadow_out', 'target_train', 'target_eval', 'test_loader'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_loaders.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 train and evaluate the target model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_target_model = Cifar10_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.9930657721757888\n",
      "Epoch 2 - Training loss: 0.9898408813476562\n",
      "Epoch 3 - Training loss: 0.9950257426738739\n",
      "Epoch 4 - Training loss: 0.9973889088630676\n",
      "Epoch 5 - Training loss: 0.9918086815357208\n",
      "Epoch 6 - Training loss: 0.9929179166316986\n",
      "Epoch 7 - Training loss: 0.9986713723540306\n",
      "Epoch 8 - Training loss: 0.9972829787969589\n",
      "Epoch 9 - Training loss: 0.9918373112916946\n",
      "Epoch 10 - Training loss: 0.9968669293642044\n",
      "Epoch 11 - Training loss: 0.9972882260322571\n",
      "Epoch 12 - Training loss: 0.9936566089391708\n",
      "Epoch 13 - Training loss: 1.0040358982563018\n",
      "Epoch 14 - Training loss: 0.9965535270810127\n",
      "Epoch 15 - Training loss: 0.9984656764984131\n",
      "Epoch 16 - Training loss: 0.9938789386034012\n",
      "Epoch 17 - Training loss: 1.0027177766084672\n",
      "Epoch 18 - Training loss: 0.9959812175512314\n",
      "Epoch 19 - Training loss: 0.9889417807340622\n",
      "Epoch 20 - Training loss: 0.9876851892709732\n"
     ]
    }
   ],
   "source": [
    "cifar_train_dataloader = cifar_loaders['target_train']\n",
    "optimizer = Adam(cifar_target_model.parameters(), lr=0.0000001)\n",
    "epochs = 20\n",
    "checkpoint_path = \"./checkpoints/case1/cifarTarget.pth\"\n",
    "target_train_loss_scores = TU.train(train_dataloader=cifar_train_dataloader, \n",
    "                                    optimizer=optimizer,\n",
    "                                    model=cifar_target_model,\n",
    "                \n",
    "                                    n_epochs=epochs,\n",
    "                                    model_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.1111, Val Accuracy: (0.607%) , precision:  0.6065, recall :  0.6065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_acc , target_precision , target_recall  = TU.eval_model(cifar_target_model,\n",
    "                                                               cifar_loaders['test_loader'],attack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results \n",
    "TU.save_training_loss(target_train_loss_scores,\"case1/cifar_target_model_train_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 train and evaluate the shadow model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_shadow_model = Cifar10_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 1.2923674753904342\n",
      "Epoch 2 - Training loss: 1.3029212895393372\n",
      "Epoch 3 - Training loss: 1.2898649885177613\n",
      "Epoch 4 - Training loss: 1.2910157915592193\n",
      "Epoch 5 - Training loss: 1.291897402739525\n",
      "Epoch 6 - Training loss: 1.286360319519043\n",
      "Epoch 7 - Training loss: 1.2983051123857499\n",
      "Epoch 8 - Training loss: 1.2907833490610123\n",
      "Epoch 9 - Training loss: 1.3014600640296936\n",
      "Epoch 10 - Training loss: 1.2954328127384185\n",
      "Epoch 11 - Training loss: 1.294667185974121\n",
      "Epoch 12 - Training loss: 1.2911283937215805\n",
      "Epoch 13 - Training loss: 1.2865879675388336\n",
      "Epoch 14 - Training loss: 1.298851662826538\n",
      "Epoch 15 - Training loss: 1.2897569953441619\n",
      "Epoch 16 - Training loss: 1.299311732339859\n",
      "Epoch 17 - Training loss: 1.2858497423171997\n",
      "Epoch 18 - Training loss: 1.2839252040863036\n",
      "Epoch 19 - Training loss: 1.2928462571620942\n",
      "Epoch 20 - Training loss: 1.2939718429803848\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = cifar_loaders['DShadow_train']\n",
    "optimizer = Adam(cifar_shadow_model.parameters(), lr=0.0000001)\n",
    "epochs = 20\n",
    "checkpoint_path = \"./checkpoints/case1/cifar_Shadow.pth\"\n",
    "shadow_train_loss_scores = TU.train(train_dataloader=train_dataloader, \n",
    "                                    optimizer=optimizer,\n",
    "                                    model=cifar_shadow_model,\n",
    "                                    n_epochs=epochs,\n",
    "                                    model_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.1112, Val Accuracy: (0.606%) , precision:  0.6057, recall :  0.6057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shadow_acc , shadow_precision , shadow_recall  = TU.eval_model(cifar_shadow_model,\n",
    "                                                               cifar_loaders['test_loader'],attack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "TU.save_training_loss(shadow_train_loss_scores,\"case1/cifar_shadow_model_train_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 train and evaluate the attack model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.1 Create dataset of the attack model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumia/cispa/ML-Leaks/datasets/data_utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "# The dataset is already created and saved in data/data/attack_dataset.csv so there s no need \n",
    "# to run this cell \n",
    "train_file_path='data/case1/cifar10/attack_train_dataset.csv'\n",
    "test_file_path='data/case1/cifar10/attack_test_dataset.csv'\n",
    "# In short the fowllong means that:\n",
    "#  Attack_train = top3(shadow(D_shadow)) = top3(shadow(D_shadwo_train + D_shadow_out_shadow)) \n",
    "result = get_attack_trainset(shadow_model=cifar_shadow_model,\n",
    "                             shadow_train_loader=cifar_loaders['DShadow_train'], \n",
    "                             shadow_out_loader=cifar_loaders['DShadow_out'],\n",
    "                             file_path=train_file_path)\n",
    "result = get_attack_trainset(shadow_model=cifar_shadow_model,\n",
    "                             shadow_train_loader=cifar_loaders['target_train'], \n",
    "                             shadow_out_loader=cifar_loaders['target_eval'],\n",
    "                             file_path=test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path='data/case1/cifar10/attack_train_dataset.csv'\n",
    "test_file_path='data/case1/cifar10/attack_test_dataset.csv'\n",
    "attack_loaders = loader.get_attack_train_test_loaders(dataset_train_path=train_file_path,\n",
    "                                                      dataset_test_path=test_file_path,\n",
    "                                                        batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3.2 train and evaluate the attack model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_model = Attack_classifier(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = attack_loaders['train_loader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.6950505030632019\n",
      "Epoch 2 - Training loss: 0.6957265115261078\n",
      "Epoch 3 - Training loss: 0.6949146528244019\n",
      "Epoch 4 - Training loss: 0.695542797613144\n",
      "Epoch 5 - Training loss: 0.6957440102815629\n",
      "Epoch 6 - Training loss: 0.6952755299568176\n",
      "Epoch 7 - Training loss: 0.6952428996801376\n",
      "Epoch 8 - Training loss: 0.6954595716953278\n",
      "Epoch 9 - Training loss: 0.6952077157020569\n",
      "Epoch 10 - Training loss: 0.6956053576946258\n",
      "Epoch 11 - Training loss: 0.6953654679775239\n",
      "Epoch 12 - Training loss: 0.6953147011995315\n",
      "Epoch 13 - Training loss: 0.6949078891038895\n",
      "Epoch 14 - Training loss: 0.6951793418169021\n",
      "Epoch 15 - Training loss: 0.6953741498470306\n",
      "Epoch 16 - Training loss: 0.6959248410224914\n",
      "Epoch 17 - Training loss: 0.6951125086069107\n",
      "Epoch 18 - Training loss: 0.6948982631444931\n",
      "Epoch 19 - Training loss: 0.695225207734108\n",
      "Epoch 20 - Training loss: 0.6953412457942962\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(attack_model.parameters(), lr=0.000001)\n",
    "\n",
    "train_scores= TU.train(train_dataloader, \n",
    "                    optimizer,\n",
    "                    model=attack_model,\n",
    "                    n_epochs=20, \n",
    "                    model_path=\"./checkpoints/case1/cifar_attack_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.0693, Val Accuracy: (0.502%) , precision:  0.5022, recall :  0.5410\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_attack,precision_attack,recall_attack = TU.eval_model(attack_model,attack_loaders['test_loader'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Case 2 : Shadow model's architecture different than the target model's architecture aka Combining attack:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mnist classifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since it's the same model I just load it again \n",
    "# This cell is not mandatory \n",
    "target_model_mnist2 = Mnist_classifier()\n",
    "target_model_mnist2.load_state_dict(torch.load( \"./checkpoints/case1/mnistTarget.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 train and evaluate the shadow model:\n",
    "Here the shadow model is a combined model of 3 models each is a submodel(Random forests , Logistic regression) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data for the sub classifiers \n",
    "data = pd.read_csv(\"./mnist csv/Mnist.csv\")\n",
    "x=train.iloc[:,1:]\n",
    "y=train.iloc[:,0]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=0)\n",
    "x_D_shadow , x_D_target,y_D_shadow , y_D_target  = train_test_split(x_train, y_train, test_size=0.5, random_state=0)\n",
    "x_D_shadow_train, x_D_shadow_out, y_D_shadow_train, y_D_shadow_out  = train_test_split(x_D_shadow, y_D_shadow, test_size=0.5, random_state=0)\n",
    "x_D_target_train, x_D_target_eval, y_D_target_train, y_D_target_eval = train_test_split(x_D_target, y_D_target, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2.1 Sub shadow model 1 : Random Forests : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators =50, random_state = 42)\n",
    "rf.fit(x_D_shadow , y_D_shadow )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       193\n",
      "           1       0.98      0.97      0.98       237\n",
      "           2       0.95      0.98      0.96       208\n",
      "           3       0.94      0.94      0.94       215\n",
      "           4       0.95      0.96      0.95       223\n",
      "           5       0.97      0.94      0.96       197\n",
      "           6       0.96      0.98      0.97       204\n",
      "           7       0.98      0.97      0.98       224\n",
      "           8       0.93      0.92      0.93       201\n",
      "           9       0.94      0.91      0.92       198\n",
      "\n",
      "    accuracy                           0.95      2100\n",
      "   macro avg       0.95      0.95      0.95      2100\n",
      "weighted avg       0.95      0.95      0.95      2100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#eval \n",
    "pred=rf.predict(x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model \n",
    "filename = './checkpoints/case2/RandomForest_mnist.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2.2 Sub shadow model  : Logistic Regression  : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_D_shadow , y_D_shadow )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95       193\n",
      "           1       0.97      0.98      0.98       237\n",
      "           2       0.92      0.91      0.91       208\n",
      "           3       0.89      0.89      0.89       215\n",
      "           4       0.92      0.88      0.90       223\n",
      "           5       0.86      0.87      0.87       197\n",
      "           6       0.89      0.95      0.92       204\n",
      "           7       0.92      0.96      0.94       224\n",
      "           8       0.88      0.84      0.86       201\n",
      "           9       0.88      0.86      0.87       198\n",
      "\n",
      "    accuracy                           0.91      2100\n",
      "   macro avg       0.91      0.91      0.91      2100\n",
      "weighted avg       0.91      0.91      0.91      2100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred2=np.floor(lr.predict(x_test))\n",
    "print(classification_report(y_test, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model \n",
    "filename = './checkpoints/case2/LogisticRegression_mnist.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2.3 Stacking submodels : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "estimators = [\n",
    "            ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
    "            ('lr', LogisticRegression())\n",
    "]\n",
    "clf = StackingClassifier(\n",
    "        estimators=estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=None,\n",
       "                   estimators=[('rf',\n",
       "                                RandomForestClassifier(bootstrap=True,\n",
       "                                                       ccp_alpha=0.0,\n",
       "                                                       class_weight=None,\n",
       "                                                       criterion='gini',\n",
       "                                                       max_depth=None,\n",
       "                                                       max_features='auto',\n",
       "                                                       max_leaf_nodes=None,\n",
       "                                                       max_samples=None,\n",
       "                                                       min_impurity_decrease=0.0,\n",
       "                                                       min_impurity_split=None,\n",
       "                                                       min_samples_leaf=1,\n",
       "                                                       min_samples_split=2,\n",
       "                                                       min_weight_fraction_leaf=0.0,\n",
       "                                                       n_estimators=50,\n",
       "                                                       n_jobs=None,...\n",
       "                                                       verbose=0,\n",
       "                                                       warm_start=False)),\n",
       "                               ('lr',\n",
       "                                LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                   dual=False,\n",
       "                                                   fit_intercept=True,\n",
       "                                                   intercept_scaling=1,\n",
       "                                                   l1_ratio=None, max_iter=100,\n",
       "                                                   multi_class='auto',\n",
       "                                                   n_jobs=None, penalty='l2',\n",
       "                                                   random_state=None,\n",
       "                                                   solver='lbfgs', tol=0.0001,\n",
       "                                                   verbose=0,\n",
       "                                                   warm_start=False))],\n",
       "                   final_estimator=None, n_jobs=None, passthrough=False,\n",
       "                   stack_method='auto', verbose=0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " clf.fit(x_D_shadow , y_D_shadow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95       193\n",
      "           1       0.99      0.97      0.98       237\n",
      "           2       0.96      0.98      0.97       208\n",
      "           3       0.93      0.94      0.94       215\n",
      "           4       0.96      0.94      0.95       223\n",
      "           5       0.97      0.95      0.96       197\n",
      "           6       0.94      0.99      0.96       204\n",
      "           7       0.98      0.97      0.98       224\n",
      "           8       0.93      0.93      0.93       201\n",
      "           9       0.93      0.92      0.92       198\n",
      "\n",
      "    accuracy                           0.95      2100\n",
      "   macro avg       0.95      0.95      0.95      2100\n",
      "weighted avg       0.95      0.95      0.95      2100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#eval \n",
    "pred3=clf.predict(x_test)\n",
    "print(classification_report(y_test, pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "filename = './checkpoints/case2/ensemble_mnist.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.3 construct Attack data : <a id=\"something\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(x1, x2, clf, path_save):\n",
    "    \"assuming x1,y1 are Dtrain shadow and x2 , y2 are Dshadow out\"\n",
    "    df = pd.DataFrame()\n",
    "    probas = clf.predict_proba(x1)\n",
    "    sorted_probs = np.sort(probas, axis=1)\n",
    "    \n",
    "    for e in sorted_probs: \n",
    "        resu = e[-3:]\n",
    "        df2 = pd.DataFrame({\"top1\": resu[0],\n",
    "                            \"top2\":resu[1],\n",
    "                            \"top3\": resu[2],\n",
    "                            \"label\": 1\n",
    "                            },index=[0])\n",
    "        df = df.append(df2)\n",
    "     \n",
    "    probas = clf.predict_proba(x2)\n",
    "    sorted_probs = np.sort(probas, axis=1)\n",
    "    for e in sorted_probs: \n",
    "        resu = e[-3:]\n",
    "        df3 = pd.DataFrame({\"top1\": resu[0],\n",
    "                            \"top2\":resu[1],\n",
    "                            \"top3\": resu[2],\n",
    "                            \"label\": 0\n",
    "                            },index=[0])\n",
    "        df = df.append(df)\n",
    "    df.to_csv(path_save,index=False)\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell has already been processed and data are saved \n",
    "# There is no need to re-run it again\n",
    "attack_train_save = \"data/case2/mnist/attack_train_dataset.csv\"\n",
    "attack_test_save = \"data/case2/mnist/attack_test_dataset.csv\"\n",
    "\n",
    "df = construct_dataset(x1=x_D_shadow_train,\n",
    "                       x2=x_D_shadow_out,\n",
    "                       clf=clf, \n",
    "                       path_save=attack_train_save)\n",
    "\n",
    "df = construct_dataset(x1=x_D_target_train,\n",
    "                       x2=x_D_target_eval,\n",
    "                       clf=clf, \n",
    "                       path_save=attack_test_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "train_file_path=\"data/case2/mnist/attack_train_dataset.csv\"\n",
    "test_file_path=\"data/case2/mnist/attack_test_dataset.csv\"\n",
    "attack_loaders = loader.get_attack_train_test_loaders(dataset_train_path=train_file_path,\n",
    "                                                      dataset_test_path=test_file_path,\n",
    "                                                        batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.4 train attack model and evaluate it : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_model = Attack_classifier(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = attack_loaders['train_loader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.6931727687517802\n",
      "Epoch 2 - Training loss: 0.6933887512164009\n",
      "Epoch 3 - Training loss: 0.6932129659449546\n",
      "Epoch 4 - Training loss: 0.6934609806627259\n",
      "Epoch 5 - Training loss: 0.6932945775507685\n",
      "Epoch 6 - Training loss: 0.6933378587390546\n",
      "Epoch 7 - Training loss: 0.6933012390794013\n",
      "Epoch 8 - Training loss: 0.6932324128640923\n",
      "Epoch 9 - Training loss: 0.6932284658415275\n",
      "Epoch 10 - Training loss: 0.6933061804986538\n",
      "Epoch 11 - Training loss: 0.6932561711858687\n",
      "Epoch 12 - Training loss: 0.6931575557641816\n",
      "Epoch 13 - Training loss: 0.6931611282484872\n",
      "Epoch 14 - Training loss: 0.6932790222622099\n",
      "Epoch 15 - Training loss: 0.6932108251672042\n",
      "Epoch 16 - Training loss: 0.6932844067277167\n",
      "Epoch 17 - Training loss: 0.6931624237756083\n",
      "Epoch 18 - Training loss: 0.6931881216235627\n",
      "Epoch 19 - Training loss: 0.6931945237898289\n",
      "Epoch 20 - Training loss: 0.6932244052863061\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(attack_model.parameters(), lr=0.0001)\n",
    "\n",
    "train_scores= TU.train(train_dataloader, \n",
    "                    optimizer,\n",
    "                    model=attack_model,\n",
    "                    n_epochs=20, \n",
    "                    model_path=\"./checkpoints/case2/mnist_attack_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.0693, Val Accuracy: (0.500%) , precision:  0.4998, recall :  0.9398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#eval \n",
    "acc_attack,precision_attack,recall_attack = TU.eval_model(attack_model,attack_loaders['test_loader'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cifar 10 classifier :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Train target model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it's the same model I just load it again \n",
    "# This cell is not mandatory \n",
    "target_model_cifar2 = Cifar10_classifier()\n",
    "target_model_cifar2.load_state_dict(torch.load( \"./checkpoints/case1/cifarTarget.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Train and test shadow model:\n",
    "Here the shadow model is a combined model of 3 models each is a submodel(Random forests , Logistic regression) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)= cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = label_binarize(y_train, classes=[0,1,2,3,4,5,6,7,8,9])\n",
    "y_test = label_binarize(y_test, classes=[0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_D_shadow , x_D_target,y_D_shadow , y_D_target  = train_test_split(x_train, y_train, test_size=0.5, random_state=0)\n",
    "x_D_shadow_train, x_D_shadow_out, y_D_shadow_train, y_D_shadow_out  = train_test_split(x_D_shadow, y_D_shadow, test_size=0.5, random_state=0)\n",
    "x_D_target_train, x_D_target_eval, y_D_target_train, y_D_target_eval = train_test_split(x_D_target, y_D_target, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_D_shadow_train = x_D_shadow_train.reshape(len(x_D_shadow_train), 3*32*32)\n",
    "x_D_shadow_out = x_D_shadow_out.reshape(len(x_D_shadow_out), 3*32*32)\n",
    "x_D_target_train = x_D_target_train.reshape(len(x_D_target_train), 3*32*32)\n",
    "x_D_target_eval= x_D_target_eval.reshape(len(x_D_target_eval), 3*32*32)\n",
    "x_test = x_test.reshape(len(x_test), 3*32*32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.1 Sub shadow model  : Logistic Regression  : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='multinomial',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=42,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_one_rest = OneVsRestClassifier(LogisticRegression(random_state=42, multi_class='multinomial'))\n",
    "lr_one_rest.fit(x_D_shadow_train, y_D_shadow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.20      0.26      1000\n",
      "           1       0.55      0.29      0.38      1000\n",
      "           2       0.21      0.06      0.10      1000\n",
      "           3       0.30      0.09      0.14      1000\n",
      "           4       0.32      0.13      0.18      1000\n",
      "           5       0.32      0.11      0.17      1000\n",
      "           6       0.45      0.22      0.29      1000\n",
      "           7       0.47      0.22      0.30      1000\n",
      "           8       0.38      0.25      0.31      1000\n",
      "           9       0.43      0.24      0.31      1000\n",
      "\n",
      "   micro avg       0.40      0.18      0.25     10000\n",
      "   macro avg       0.38      0.18      0.24     10000\n",
      "weighted avg       0.38      0.18      0.24     10000\n",
      " samples avg       0.15      0.18      0.16     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#test \n",
    "pred4=lr_one_rest.predict(x_test)\n",
    "print(classification_report(y_test, pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model \n",
    "filename = './checkpoints/case2/LogisticRegression_Cifar.sav'\n",
    "pickle.dump(lr_one_rest, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.2 Sub shadow model  : Random forest : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Random forests \n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(x_D_shadow_train, y_D_shadow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.22      0.35       258\n",
      "           1       0.75      0.01      0.02       252\n",
      "           2       0.00      0.00      0.00       236\n",
      "           3       0.00      0.00      0.00       286\n",
      "           4       0.50      0.01      0.02       221\n",
      "           5       0.88      0.02      0.05       282\n",
      "           6       0.71      0.02      0.04       244\n",
      "           7       0.67      0.01      0.02       240\n",
      "           8       0.78      0.22      0.34       233\n",
      "           9       0.86      0.02      0.05       248\n",
      "\n",
      "   micro avg       0.78      0.05      0.10      2500\n",
      "   macro avg       0.59      0.05      0.09      2500\n",
      "weighted avg       0.59      0.05      0.09      2500\n",
      " samples avg       0.05      0.05      0.05      2500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Eval model \n",
    "pred5=random_forest.predict(x_test)\n",
    "print(classification_report(y_test, pred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model \n",
    "filename = './checkpoints/case2/RandomForest_Cifar.sav'\n",
    "pickle.dump(random_forest, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.3 Stacking submodels : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "estimators = [\n",
    "            ('lr', OneVsRestClassifier(LogisticRegression(random_state=42,\n",
    "                                                          multi_class='multinomial',\n",
    "                                                         solver='sag',max_iter=10))),\n",
    "            ('rf',RandomForestClassifier(random_state=42))\n",
    "    \n",
    "            ]\n",
    "clf2 = StackingClassifier(estimators=estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf2.fit(x_D_shadow_train, y_D_shadow_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.22      0.35       258\n",
      "           1       0.75      0.01      0.02       252\n",
      "           2       0.00      0.00      0.00       236\n",
      "           3       0.00      0.00      0.00       286\n",
      "           4       0.50      0.01      0.02       221\n",
      "           5       0.88      0.02      0.05       282\n",
      "           6       0.71      0.02      0.04       244\n",
      "           7       0.67      0.01      0.02       240\n",
      "           8       0.78      0.22      0.34       233\n",
      "           9       0.86      0.02      0.05       248\n",
      "\n",
      "   micro avg       0.78      0.05      0.10      2500\n",
      "   macro avg       0.59      0.05      0.09      2500\n",
      "weighted avg       0.59      0.05      0.09      2500\n",
      " samples avg       0.05      0.05      0.05      2500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soumia/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Eval model \n",
    "pred6=clf2.predict(x_test)\n",
    "print(classification_report(y_test, pred6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model \n",
    "filename = './checkpoints/case2/stacked_models_Cifar.sav'\n",
    "pickle.dump(clf2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 construct Attack data : \n",
    "Make sure to run the cell of [This function](#Something) Before continuing in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 12500, 2)\n",
      "(12500, 10)\n",
      "[0.02433537 0.0244615  0.28429316 0.21142529 0.08037525 0.28627002\n",
      " 0.05598005 0.02341974 0.10610243 0.00353619]\n"
     ]
    }
   ],
   "source": [
    "##### print(len(x_D_shadow_out))\n",
    "probas = random_forest.predict_proba(x_D_shadow_out)\n",
    "pipo = np.asarray(probas) \n",
    "print(pipo.shape)\n",
    "maxos =  np.argmax(pipo, axis=2)\n",
    "p = maxos.reshape(12500,10)\n",
    "\n",
    "probs2 = lr_one_rest.predict_proba(x_D_shadow_out)\n",
    "print(p.shape)\n",
    "print(probs2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 12500)\n",
      "(12500, 10)\n"
     ]
    }
   ],
   "source": [
    "print(maxos.shape)\n",
    "print(probs2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell has already been processed and data are saved \n",
    "# There is no need to re-run it again\n",
    "attack_train_save = \"data/case2/cifar10/attack_train_dataset.csv\"\n",
    "attack_test_save = \"data/case2/cifar10/attack_test_dataset.csv\"\n",
    "\n",
    "df = construct_dataset(x1=x_D_shadow_train,\n",
    "                       x2=x_D_shadow_out,\n",
    "                       clf=clf2, \n",
    "                       path_save=attack_train_save)\n",
    "\n",
    "df = construct_dataset(x1=x_D_target_train,\n",
    "                       x2=x_D_target_eval,\n",
    "                       clf=clf2, \n",
    "                       path_save=attack_test_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path='data/case2/cifar10/attack_train_dataset.csv'\n",
    "test_file_path='data/case2/cifar10/attack_test_dataset.csv'\n",
    "attack_loaders = loader.get_attack_train_test_loaders(dataset_train_path=train_file_path,\n",
    "                                                      dataset_test_path=test_file_path,\n",
    "                                                        batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.4 train attack model and evaluate it : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_model = Attack_classifier(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = attack_loaders['train_loader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.6968972384214401\n",
      "Epoch 2 - Training loss: 0.6964005449771881\n",
      "Epoch 3 - Training loss: 0.697587501335144\n",
      "Epoch 4 - Training loss: 0.6964075799226761\n",
      "Epoch 5 - Training loss: 0.6963178188323974\n",
      "Epoch 6 - Training loss: 0.6969622575044632\n",
      "Epoch 7 - Training loss: 0.696983782529831\n",
      "Epoch 8 - Training loss: 0.6966662009954453\n",
      "Epoch 9 - Training loss: 0.6969615666627884\n",
      "Epoch 10 - Training loss: 0.6962411234855652\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(attack_model.parameters(), lr=0.000001)\n",
    "\n",
    "train_scores= TU.train(train_dataloader, \n",
    "                    optimizer,\n",
    "                    model=attack_model,\n",
    "                    n_epochs=10, \n",
    "                    model_path=\"./checkpoints/case2/cifar_attack_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.0693, Val Accuracy: (0.498%) , precision:  0.4972, recall :  0.3442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_attack,precision_attack,recall_attack = TU.eval_model(attack_model,attack_loaders['test_loader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
