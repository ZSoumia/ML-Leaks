{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Leaks Paper implementation:\n",
    "\n",
    ">This is an implementation of adversary 1 scenario of the paper (ML-Leaks)[https://arxiv.org/pdf/1806.01246.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. General Idea of the paper:\n",
    "\n",
    "As machine learning as a service (Mlaas) is getting widely used, privacy issues in this setting are also getting a lot of attention. One of the most famous attacks that a depolyed machine learning model can be victim of is \"the membership inference attack\". This attack allows an adversary to know if a particular data point was used to trained a given model. In case of a model trained on highly private data, this attack can be a real danger.\n",
    "\n",
    "Early demos of memebership infrence attacks assumed the following :\n",
    "\n",
    "- Knowledge of the target model architecture.\n",
    "\n",
    "- Using multiple shadow models.\n",
    "\n",
    "- Having shadow models trained on datasets that are from the same distribution of the dataset used to train the target model itself.\n",
    "\n",
    "Despite the high performance of such attacks, the existence of these assumptions made it unrealistic to be deployed in real world. In this paper, the authors relaxed these assumptions and suggested some effective defense strategies , namely : Dropout and model stacking.\n",
    "\n",
    "The authors designed 3 adversary strategies :\n",
    "\n",
    "- Adversary 1 : Used only one shadow model + no knowledge of the classification algorithm used. ==> The usage of only one shadow model resulted in a similar performance compared to using many shadow models. This can make the attacks much more computationally efficient.\n",
    "\n",
    "- Adversary 2 : The data sued to train the shadow model isn't necessarily from the same distribution of the data used to train the target model ==> This makes the attack more powerful and realistic.\n",
    "\n",
    "- Adversary 3 : No shadow models are used + an attack in an unsupervised attack ==> despite the drop of the performance for this attck it is still effective.\n",
    "\n",
    "> In this notebook we are implementing the Adversary 1 and testing on Mnist dataset and Cifar10.\n",
    "\n",
    "### Table of Content:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important note : The number of epochs displayed when training may not be the exact number of training epochs used for the final model. By the time I was running tests , it happened when I needed to add epochs and run again the training cell ( to improuve the accuracy/loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages \n",
    "from models.models import *\n",
    "import dataloaders as loader\n",
    "import training_utils as TU\n",
    "from torch.optim import *\n",
    "from datasets.data_utils import *\n",
    "from datasets.attack_dataset import *  \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Case 1 : Same model architecture for the shadow and the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Mnist classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Load data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = loader.get_dataloaders(batch_size=128,dataset_name=\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DShadow_train', 'DShadow_out', 'target_train', 'target_eval', 'test_loader'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 train and evaluate the target model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Mnist_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 1.7336031607652114\n",
      "Epoch 2 - Training loss: 0.6991646052417109\n",
      "Epoch 3 - Training loss: 0.43768555153224425\n",
      "Epoch 4 - Training loss: 0.3306552916765213\n",
      "Epoch 5 - Training loss: 0.2674894731933788\n",
      "Epoch 6 - Training loss: 0.22669988011909745\n",
      "Epoch 7 - Training loss: 0.19536167168516225\n",
      "Epoch 8 - Training loss: 0.17107124607694352\n",
      "Epoch 9 - Training loss: 0.15237095478480145\n",
      "Epoch 10 - Training loss: 0.13591645995817953\n",
      "Epoch 11 - Training loss: 0.12330653928851677\n",
      "Epoch 12 - Training loss: 0.1126803884251138\n",
      "Epoch 13 - Training loss: 0.10286486606602951\n",
      "Epoch 14 - Training loss: 0.09479775257661181\n",
      "Epoch 15 - Training loss: 0.08915792882316194\n",
      "Epoch 16 - Training loss: 0.08216708196106098\n",
      "Epoch 17 - Training loss: 0.07772603141680612\n",
      "Epoch 18 - Training loss: 0.0728833022195909\n",
      "Epoch 19 - Training loss: 0.06714723495987512\n",
      "Epoch 20 - Training loss: 0.0630572308012742\n",
      "Epoch 21 - Training loss: 0.059463714375713114\n",
      "Epoch 22 - Training loss: 0.055863706426600275\n",
      "Epoch 23 - Training loss: 0.052924576207553434\n",
      "Epoch 24 - Training loss: 0.05075882353930403\n",
      "Epoch 25 - Training loss: 0.04710791998353424\n",
      "Epoch 26 - Training loss: 0.045075532830304514\n",
      "Epoch 27 - Training loss: 0.04265704567117964\n",
      "Epoch 28 - Training loss: 0.040445889527830535\n",
      "Epoch 29 - Training loss: 0.03859937981844454\n",
      "Epoch 30 - Training loss: 0.03658336338634461\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = loaders['target_train']\n",
    "optimizer = Adam(target_model.parameters(), lr=0.0001)\n",
    "epochs = 30\n",
    "checkpoint_path = \"./checkpoints/mnistTarget.pth\"\n",
    "target_train_loss_scores = TU.train(train_dataloader=train_dataloader, \n",
    "                                    optimizer=optimizer,\n",
    "                                    model=target_model,\n",
    "                                    n_epochs=epochs,\n",
    "                                    model_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.0004, Val Accuracy: 9846/10000 (98.460%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "acc = TU.eval_model(target_model,loaders['test_loader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results \n",
    "TU.save_training_loss(target_train_loss_scores,\"case1/target_model_train_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 train and evaluate the shadow model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_model = Mnist_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 1.650734110403869\n",
      "Epoch 2 - Training loss: 0.6618791730222056\n",
      "Epoch 3 - Training loss: 0.4201601612365852\n",
      "Epoch 4 - Training loss: 0.31804574590365764\n",
      "Epoch 5 - Training loss: 0.25947067652971056\n",
      "Epoch 6 - Training loss: 0.22189391688522647\n",
      "Epoch 7 - Training loss: 0.19272407453696608\n",
      "Epoch 8 - Training loss: 0.16948357173952006\n",
      "Epoch 9 - Training loss: 0.15162894909538455\n",
      "Epoch 10 - Training loss: 0.13634572556968463\n",
      "Epoch 11 - Training loss: 0.12467115845973209\n",
      "Epoch 12 - Training loss: 0.1144538798953517\n",
      "Epoch 13 - Training loss: 0.1062625774766429\n",
      "Epoch 14 - Training loss: 0.09781695144661402\n",
      "Epoch 15 - Training loss: 0.09179458075787052\n",
      "Epoch 16 - Training loss: 0.08455126115389294\n",
      "Epoch 17 - Training loss: 0.07867810913061692\n",
      "Epoch 18 - Training loss: 0.07313994658416358\n",
      "Epoch 19 - Training loss: 0.06843136344105005\n",
      "Epoch 20 - Training loss: 0.06478635789984363\n",
      "Epoch 21 - Training loss: 0.061030569454749765\n",
      "Epoch 22 - Training loss: 0.05771656714821771\n",
      "Epoch 23 - Training loss: 0.05420634732187047\n",
      "Epoch 24 - Training loss: 0.05107042243135935\n",
      "Epoch 25 - Training loss: 0.04791683941227147\n",
      "Epoch 26 - Training loss: 0.0456683318746292\n",
      "Epoch 27 - Training loss: 0.04273505269754994\n",
      "Epoch 28 - Training loss: 0.041465378397980986\n",
      "Epoch 29 - Training loss: 0.03880343389681588\n",
      "Epoch 30 - Training loss: 0.03650453166594192\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = loaders['DShadow_train']\n",
    "optimizer = Adam(shadow_model.parameters(), lr=0.0001)\n",
    "epochs = 30\n",
    "checkpoint_path = \"./checkpoints/mnistShadow.pth\"\n",
    "shadow_train_loss_scores = TU.train(train_dataloader=train_dataloader, \n",
    "                                    optimizer=optimizer,\n",
    "                                    model=shadow_model,\n",
    "                                    n_epochs=epochs,\n",
    "                                    model_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.0004, Val Accuracy: 9825/10000 (98.250%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_shadow = TU.eval_model(shadow_model,loaders['test_loader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results \n",
    "TU.save_training_loss(shadow_train_loss_scores,\"case1/shadow_model_train_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 train and evaluate the attack model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.1 Create dataset of the attack model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The dataset is already created and saved in data/data/attack_dataset.csv so there s no need \n",
    "# to run this cell \n",
    "train_file_path='data/attack_train_dataset.csv'\n",
    "test_file_path='data/attack_test_dataset.csv'\n",
    "# In short the fowllong means that:\n",
    "#  Attack_train = top3(shadow(D_shadow)) = top3(shadow(D_shadwo_train + D_shadow_out_shadow)) \n",
    "result = get_attack_trainset(shadow_model=shadow_model,\n",
    "                             shadow_train_loader=loaders['DShadow_train'], \n",
    "                             shadow_out_loader=loaders['DShadow_out'],\n",
    "                             file_path=train_file_path)\n",
    "result = get_attack_trainset(shadow_model=shadow_model,\n",
    "                             shadow_train_loader=loaders['target_train'], \n",
    "                             shadow_out_loader=loaders['target_eval'],\n",
    "                             file_path=test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path='data/attack_train_dataset.csv'\n",
    "test_file_path='data/attack_test_dataset.csv'\n",
    "attack_loaders = loader.get_attack_train_test_loaders(dataset_train_path=train_file_path,\n",
    "                                                      dataset_test_path=test_file_path,\n",
    "                                                        batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.2 train and evaluate the attack model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_model = Attack_classifier(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = attack_loaders['train_loader']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.6961096758445104\n",
      "Epoch 2 - Training loss: 0.6956390447219213\n",
      "Epoch 3 - Training loss: 0.6959529554247856\n",
      "Epoch 4 - Training loss: 0.6956756233970324\n",
      "Epoch 5 - Training loss: 0.6961275424361228\n",
      "Epoch 6 - Training loss: 0.6961167560418446\n",
      "Epoch 7 - Training loss: 0.696594585955143\n",
      "Epoch 8 - Training loss: 0.6959110643863679\n",
      "Epoch 9 - Training loss: 0.6963876638611157\n",
      "Epoch 10 - Training loss: 0.6960333439906439\n",
      "Epoch 11 - Training loss: 0.6956823712587357\n",
      "Epoch 12 - Training loss: 0.6963086466987928\n",
      "Epoch 13 - Training loss: 0.6951838530699412\n",
      "Epoch 14 - Training loss: 0.6957121727267901\n",
      "Epoch 15 - Training loss: 0.6960745484630266\n",
      "Epoch 16 - Training loss: 0.6964540767272314\n",
      "Epoch 17 - Training loss: 0.6954973247647286\n",
      "Epoch 18 - Training loss: 0.6959402126471201\n",
      "Epoch 19 - Training loss: 0.6955236923098564\n",
      "Epoch 20 - Training loss: 0.6957927646835645\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(attack_model.fc1.parameters(), lr=0.00001)\n",
    "\n",
    "train_scores= TU.train(train_dataloader, \n",
    "                    optimizer,\n",
    "                    model=attack_model,\n",
    "                    n_epochs=20, \n",
    "                    model_path=\"./checkpoints/attack_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.0693, Val Accuracy: (0.499%) , precision:  0.4994, recall :  0.8363\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_attack,precision_attack,recall_attack = TU.eval_model(attack_model,attack_loaders['test_loader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results \n",
    "TU.save_training_loss(train_scores,\"case1/attack_model_train_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "raw_data = pd.read_csv('data/attack_train_dataset.csv')\n",
    "x = raw_data[['top1', 'top2', 'top3']]\n",
    "y = raw_data['label']\n",
    "test_data = pd.read_csv(\"data/attack_test_dataset.csv\")\n",
    "x_test = test_data[['top1', 'top2', 'top3']]\n",
    "y_test = test_data['label']\n",
    "model = LogisticRegression()\n",
    "model.fit(x, y)\n",
    "predictions = model.predict(x_test)\n",
    "print(len(predictions))\n",
    "l = classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cifar10 classifier :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Load data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_loaders = loader.get_dataloaders(batch_size=5,dataset_name=\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DShadow_train', 'DShadow_out', 'target_train', 'target_eval', 'test_loader'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_loaders.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 train and evaluate the target model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_target_model = Cifar10_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 1.7812209156036376\n",
      "Epoch 2 - Training loss: 1.772486675262451\n",
      "Epoch 3 - Training loss: 1.7700185741901397\n",
      "Epoch 4 - Training loss: 1.7641615063667297\n",
      "Epoch 5 - Training loss: 1.757532771372795\n",
      "Epoch 6 - Training loss: 1.7487286954879762\n",
      "Epoch 7 - Training loss: 1.7463976012706757\n",
      "Epoch 8 - Training loss: 1.7426651539802551\n",
      "Epoch 9 - Training loss: 1.7424202445983887\n",
      "Epoch 10 - Training loss: 1.7404421015501021\n",
      "Epoch 11 - Training loss: 1.7397437626361847\n",
      "Epoch 12 - Training loss: 1.7314544401407241\n",
      "Epoch 13 - Training loss: 1.7294480040788651\n",
      "Epoch 14 - Training loss: 1.7362733224630356\n",
      "Epoch 15 - Training loss: 1.729334351325035\n",
      "Epoch 16 - Training loss: 1.7251744873762132\n",
      "Epoch 17 - Training loss: 1.722428078699112\n",
      "Epoch 18 - Training loss: 1.722384524011612\n",
      "Epoch 19 - Training loss: 1.7164879006147384\n",
      "Epoch 20 - Training loss: 1.7116438692092895\n"
     ]
    }
   ],
   "source": [
    "cifar_train_dataloader = cifar_loaders['target_train']\n",
    "optimizer = Adam(cifar_target_model.parameters(), lr=0.00001)\n",
    "epochs = 20\n",
    "checkpoint_path = \"./checkpoints/case1/cifarTarget.pth\"\n",
    "target_train_loss_scores = TU.train(train_dataloader=cifar_train_dataloader, \n",
    "                                    optimizer=optimizer,\n",
    "                                    model=cifar_target_model,\n",
    "                                    n_epochs=epochs,\n",
    "                                    model_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.2963, Val Accuracy: (0.474%) , precision:  0.4741, recall :  0.4741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "target_acc , target_precision , target_recall  = TU.eval_model(cifar_target_model,\n",
    "                                                               cifar_loaders['test_loader'],attack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results \n",
    "TU.save_training_loss(target_train_loss_scores,\"case1/cifar_target_model_train_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 train and evaluate the shadow model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_shadow_model = Mnist_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = cifar_loaders['DShadow_train']\n",
    "optimizer = Adam(cifar_shadow_model.parameters(), lr=0.0001)\n",
    "epochs = 500\n",
    "checkpoint_path = \"./checkpoints/case1/cifar_Shadow.pth\"\n",
    "shadow_train_loss_scores = TU.train(train_dataloader=train_dataloader, \n",
    "                                    optimizer=optimizer,\n",
    "                                    model=cifar_shadow_model,\n",
    "                                    n_epochs=epochs,\n",
    "                                    model_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Case 2 : Shadow model's architecture different than the target model's architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mnist classifier :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cifar 10 classifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
